{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('database.sqlite')\n",
    "\n",
    "filtered_data = pd.read_sql_query(\"\"\"\n",
    "SELECT * \n",
    "FROM Reviews\n",
    "WHERE Score!=3\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_score = filtered_data['Score']\n",
    "\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "\n",
    "filtered_data['Score']=filtered_data['Score'].map(partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting the Data based on Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = filtered_data.sort_values(\"Time\",axis=0,ascending=True)\n",
    "\n",
    "final = filtered_data.drop_duplicates(subset={'UserId','Time','ProfileName','Text'},keep='first',inplace=False)\n",
    "\n",
    "#### Duplicate datas have been discarded\n",
    "\n",
    "final = final[final.HelpfulnessNumerator<=final.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Ceaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "stop.remove('not')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma =WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "def cleanpunc(sentence): #function to clean the word of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|%|!|+|*|@|&|^|`|~|\\'|\"|#|=]',r'',sentence)\n",
    "    cleaned = re.sub(r'[:|;|.|)|(|,|\\|/|_|-]',r' ',cleaned)\n",
    "    cleaned = re.sub(r'\\s+',r' ',cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def cleannewline(sentance):\n",
    "    cleaned = re.sub(r'\\n',r' ',sentance)\n",
    "    return cleaned\n",
    "\n",
    "def cleannumbers(sentance):\n",
    "    cleaned=re.sub(r'\\s\\d*',r' ',sentance)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "\n",
      "\n",
      "Time taken for text pre processing:  293.24802999999997\n",
      "\n",
      "\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "t0 = time.clock()\n",
    "i=0\n",
    "str1=''\n",
    "final_string = []\n",
    "all_positive_words = []\n",
    "all_negative_words = []\n",
    "s = ''\n",
    "for sent in final['Text'].values:\n",
    "    filtered_sentence = []\n",
    "    sent = cleanhtml(sent)\n",
    "    sent=cleannumbers(sent)\n",
    "    sent=cleannewline(sent)\n",
    "    for w in sent.split(\" \"):\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if((cleaned_words.isalpha())&(len(cleaned_words)>2)):\n",
    "                if(cleaned_words.lower() not in stop):\n",
    "                    s=lemma.lemmatize(cleaned_words.lower(),'v')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if (final['Score'].values)[i]=='positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if (final['Score'].values)[i]=='negative':\n",
    "                        all_negative_words.append(s)\n",
    "                else:\n",
    "                    continue\n",
    "        else:\n",
    "            continue\n",
    "    str1 = \" \".join(filtered_sentence)\n",
    "    final_string.append(str1)\n",
    "    i+=1\n",
    "print(\"=\"*120)\n",
    "print('\\n')\n",
    "print(\"Time taken for text pre processing: \",time.clock()-t0)\n",
    "print('\\n')\n",
    "print(\"=\"*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_string)\n",
    "\n",
    "final_df['Time']=final['Time']\n",
    "\n",
    "final_df['Score']=final['Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.dropna(axis=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['Score']=final_df['Score'].map({'positive':1,'negative':-1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Time</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buy several vitality can dog food products fin...</td>\n",
       "      <td>1.303862e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>product arrive label jumbo salt peanuts peanut...</td>\n",
       "      <td>1.346976e+09</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>1.219018e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>look secret ingredient robitussin believe find...</td>\n",
       "      <td>1.307923e+09</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>1.350778e+09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          Time  Score\n",
       "0  buy several vitality can dog food products fin...  1.303862e+09      1\n",
       "1  product arrive label jumbo salt peanuts peanut...  1.346976e+09     -1\n",
       "2  confection around centuries light pillowy citr...  1.219018e+09      1\n",
       "3  look secret ingredient robitussin believe find...  1.307923e+09     -1\n",
       "4  great taffy great price wide assortment yummy ...  1.350778e+09      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=final_df[0].values\n",
    "y=final_df[\"Score\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting Processed Data into 70% Train set and 30% Test set (From top 100k Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1=X[0:70000]\n",
    "y_1=y[0:70000]\n",
    "X_test=X[70000:100000]\n",
    "y_test=y[70000:100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(min_df=10)\n",
    "final_counts_X1 = count_vect.fit_transform(X_1)\n",
    "#### Got Bag of Words for X-train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_counts_Xtest= count_vect.transform(X_test)\n",
    "#### Got Bag of Words for X-train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving bow train,test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"count_vect.txt\", \"wb\") as fp:\n",
    "    pickle.dump(count_vect, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_counts_X1.txt\", \"wb\") as fp:\n",
    "    pickle.dump(final_counts_X1, fp)\n",
    "with open(\"final_counts_Xtest.txt\", \"wb\") as fp:\n",
    "    pickle.dump(final_counts_Xtest, fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tfidf Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(min_df=10)\n",
    "final_tfidf_X1=tfidf_vect.fit_transform(X_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tfidf_Xtest=tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving bow train,test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tfidf_vect.txt\", \"wb\") as fp:\n",
    "    pickle.dump(tfidf_vect, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz('final_tfidf_X1.npz',final_tfidf_X1)\n",
    "sparse.save_npz('final_tfidf_Xtest.npz',final_tfidf_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_tfidf_X1.txt\", \"wb\") as fp:\n",
    "    pickle.dump(final_tfidf_X1, fp)\n",
    "with open(\"final_tfidf_Xtest.txt\", \"wb\") as fp:\n",
    "    pickle.dump(final_tfidf_Xtest, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sent_X1=[]\n",
    "for sent in X_1:\n",
    "    list_of_sent_X1.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sent_Xtest=[]\n",
    "for sent in X_test:\n",
    "    list_of_sent_Xtest.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_count = 5 considers only words that occured atleast 5 times\n",
    "w2v_model=gensim.models.Word2Vec(list_of_sent_X1,min_count=5,size=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  15535\n",
      "sample words  ['bought', 'several', 'vitality', 'canned', 'dog', 'food', 'products', 'found', 'good', 'quality', 'product', 'looks', 'like', 'stew', 'processed', 'meat', 'smells', 'better', 'labrador', 'finicky', 'appreciates', 'arrived', 'labeled', 'jumbo', 'salted', 'peanuts', 'actually', 'small', 'sized', 'unsalted', 'not', 'sure', 'error', 'vendor', 'intended', 'represent', 'confection', 'around', 'centuries', 'light', 'citrus', 'gelatin', 'nuts', 'case', 'cut', 'tiny', 'squares', 'liberally', 'coated', 'powdered']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Avg W2V, TFIDF-W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_feat = tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tfidf_X1 = []\n",
    "row=0;\n",
    "for sent in list_of_sent_X1: # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            if word in tfidf_feat:\n",
    "                tf_idf = final_tfidf_X1[row, tfidf_feat.index(word)]\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                weight_sum += tf_idf\n",
    "            else:\n",
    "                sent_vec += np.zeros(50)\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    w2v_tfidf_X1.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tfidf_Xtest = []\n",
    "row=0;\n",
    "for sent in list_of_sent_Xtest: # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            # obtain the tf_idfidf of a word in a sentence/review\n",
    "            if word in tfidf_feat:\n",
    "                tf_idf = final_tfidf_X1[row, tfidf_feat.index(word)]\n",
    "                sent_vec += (vec * tf_idf)\n",
    "                weight_sum += tf_idf\n",
    "            else:\n",
    "                sent_vec += np.zeros(50)\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    w2v_tfidf_Xtest.append(sent_vec)\n",
    "    row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_tfidf_Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_avg_X1 = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent_X1: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    w2v_avg_X1.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_avg_Xtest = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in list_of_sent_Xtest: # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    w2v_avg_Xtest.append(sent_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.86153142e-01,  1.32833720e-02,  2.99984367e-02, -5.22968030e-01,\n",
       "        4.30802614e-01, -7.31527222e-02,  2.46449096e-01, -3.66886729e-01,\n",
       "        9.95677706e-01,  1.11546915e-01, -5.34215889e-01, -1.91905060e-01,\n",
       "       -2.86802006e-01, -7.60731974e-02, -9.19047531e-01, -7.69377659e-01,\n",
       "        5.84753209e-01, -6.93941044e-01,  1.73531016e-02, -2.22409021e-01,\n",
       "       -3.62268700e-01, -4.77278809e-01,  2.41755021e-01,  5.42519509e-01,\n",
       "       -1.54795681e-01,  3.13964551e-04, -4.22534948e-01,  2.72421488e-01,\n",
       "       -1.24373217e-01, -1.05407354e+00,  4.42179748e-02, -2.61425074e-01,\n",
       "        5.56642671e-01,  1.28112638e-01, -1.02148023e+00,  3.41930488e-01,\n",
       "       -6.47521118e-01, -2.21532780e-01,  7.46036242e-01,  2.52206292e-01,\n",
       "        5.53603931e-01,  1.08071832e+00,  7.66530347e-01,  3.89471272e-01,\n",
       "       -1.95611678e-01, -6.06795925e-01, -2.11925665e-01, -9.95652612e-02,\n",
       "       -7.89889621e-01,  4.39070828e-01])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_avg_X1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving avg_w2v and tfidf_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"w2v_tfidf_X1.txt\", \"wb\") as fp:\n",
    "    pickle.dump(w2v_tfidf_X1, fp)\n",
    "with open(\"w2v_tfidf_Xtest.txt\", \"wb\") as fp:\n",
    "    pickle.dump(w2v_tfidf_Xtest, fp)\n",
    "with open(\"w2v_avg_X1.txt\", \"wb\") as fp:\n",
    "    pickle.dump(w2v_avg_X1, fp)\n",
    "with open(\"w2v_avg_Xtest.txt\", \"wb\") as fp:\n",
    "    pickle.dump(w2v_avg_Xtest, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"w2v_model.txt\", \"wb\") as fp:\n",
    "    pickle.dump(w2v_model, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Loading the w2v matrices\\n\\nwith open(\"w2v_tfidf_X1.txt\", \"rb\") as fp:\\n    w2v_tfidf_X1 = pickle.load(fp)\\nwith open(\"w2v_tfidf_Xtest.txt\", \"rb\") as fp:\\n    w2v_tfidf_Xtest = pickle.load(fp)\\nwith open(\"w2v_avg_X1.txt\", \"rb\") as fp:\\n    w2v_avg_X1 = pickle.load(fp)\\nwith open(\"w2v_avg_Xtest.txt\", \"rb\") as fp:\\n    w2v_avg_Xtest = pickle.load(fp)\\n\\n\\n#loading bow matrices\\n\\nfinal_counts_X1 = sparse.load_npz(\\'final_counts_X1.npz\\')\\nfinal_counts_Xtest = sparse.load_npz(\\'final_counts_Xtest.npz\\')\\n\\n#loading tfidf matrices\\nfinal_tfidf_X1 = sparse.load_npz(\\'final_tfidf_X1.npz\\')\\nfinal_tfidf_Xtest = sparse.load_npz(\\'final_tfidf_Xtest.npz\\')\\n\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Loading the w2v matrices\n",
    "\n",
    "with open(\"w2v_tfidf_X1.txt\", \"rb\") as fp:\n",
    "    w2v_tfidf_X1 = pickle.load(fp)\n",
    "with open(\"w2v_tfidf_Xtest.txt\", \"rb\") as fp:\n",
    "    w2v_tfidf_Xtest = pickle.load(fp)\n",
    "with open(\"w2v_avg_X1.txt\", \"rb\") as fp:\n",
    "    w2v_avg_X1 = pickle.load(fp)\n",
    "with open(\"w2v_avg_Xtest.txt\", \"rb\") as fp:\n",
    "    w2v_avg_Xtest = pickle.load(fp)\n",
    "\n",
    "\n",
    "#loading bow matrices\n",
    "\n",
    "final_counts_X1 = sparse.load_npz('final_counts_X1.npz')\n",
    "final_counts_Xtest = sparse.load_npz('final_counts_Xtest.npz')\n",
    "\n",
    "#loading tfidf matrices\n",
    "final_tfidf_X1 = sparse.load_npz('final_tfidf_X1.npz')\n",
    "final_tfidf_Xtest = sparse.load_npz('final_tfidf_Xtest.npz')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"y_1.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_1, fp)\n",
    "with open(\"y_test.txt\", \"wb\") as fp:\n",
    "    pickle.dump(y_test, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the target vectors\n",
    "'''\n",
    "\n",
    "with open(\"y_1.txt\", \"rb\") as fp:\n",
    "    y_1 = pickle.load(fp)\n",
    "with open(\"y_test.txt\", \"rb\") as fp:\n",
    "    y_test = pickle.load(fp)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20608"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT=DecisionTreeClassifier()\n",
    "DT.fit(final_counts_X1,y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8443724312361682"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_test,DT.predict(final_counts_Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
